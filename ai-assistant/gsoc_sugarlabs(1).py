# -*- coding: utf-8 -*-
"""gsoc-sugarlabs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ULCtsNzcUomFMV1Ot_P2flg1btjh-0L-
"""

!pip install transformers torch accelerate sentencepiece huggingface_hub huggingface-hub
!pip install -q -U bitsandbytes
!pip install -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git

from transformers import AutoTokenizer, AutoModelForCausalLM

def correct_text(text):
    model_id = "microsoft/phi-2"

    # Load tokenizer and model with trust_remote_code enabled
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True
    ).to("cuda")  # Explicitly move model to GPU

    prompt = (
        "Your task is to correct the following sentence for all grammatical errors. "
        "Return ONLY the corrected sentence with no additional text, explanation, or labels. "
        "Do not include any punctuation or text that is not part of the corrected sentence. "
        "Sentence to correct: \"{}\" "
        "Corrected sentence:"
    ).format(text)

    # Tokenize input and move tensors to GPU
    inputs = tokenizer(text, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=100)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the corrected part (Gemma sometimes repeats the prompt)
    if "Corrected:" in decoded:
        corrected = decoded.split("Corrected:")[-1].strip(' "\n')
    else:
        corrected = decoded
    return corrected

# Test the function
print(correct_text("She go to school."))  # Expected output: "She goes to school."

!pip install fastapi uvicorn pyngrok

import threading
import time
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import requests
import nest_asyncio

nest_asyncio.apply()  # Required for nested event loops in Colab

# Define the FastAPI app and model loading as before
app = FastAPI()
model_id = "microsoft/phi-2"

# Load tokenizer and model in half precision with device mapping offloading if needed
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"  # Offload layers automatically if GPU memory is low
)

class TextRequest(BaseModel):
    text: str

@app.post("/correct")
async def correct_text_endpoint(request: TextRequest):
    # Enhanced prompt: instruct the model explicitly not to echo any instructions.
    prompt = (
        "Your task is to correct the following sentence for all grammatical errors. "
        "Return ONLY the corrected sentence with no additional text, explanation, or labels. "
        "Do not include any instructions or extra punctuation beyond what is necessary for the corrected sentence. "
        "Sentence to correct: \"{text}\" "
        "Corrected sentence:"
    ).format(text=request.text)

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=250)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Use regex to extract only the text after "Corrected sentence:"
    match = re.search(r'Corrected sentence:\s*(.*)', decoded)
    if match:
        corrected_text = match.group(1).strip()
    else:
        corrected_text = decoded.strip()

    return {"corrected": corrected_text}

# Function to run the server using uvicorn
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Start the server in a background thread
server_thread = threading.Thread(target=run_server, daemon=True)
server_thread.start()

# Give the server some time to start
time.sleep(5)

# Now, send the POST request from the same cell
response = requests.post(
    "http://localhost:8000/correct",
    json={"text": "He don't know nothing about his school."}
)
print("Response:", response.json())

import nest_asyncio
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pyngrok import ngrok
import re
from fastapi.middleware.cors import CORSMiddleware
nest_asyncio.apply()

# Load model
model_id = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Or specify your origin like ["https://hoppscotch.io"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class TextRequest(BaseModel):
    text: str

@app.post("/correct")
async def correct_text_endpoint(request: TextRequest):
    prompt = (
        "Your task is to correct the following sentence for all grammatical errors. "
        "Return ONLY the corrected sentence with no additional text, explanation, or labels. "
        "Do not include any instructions or extra punctuation beyond what is necessary for the corrected sentence. "
        f"Sentence to correct: \"{request.text}\" Corrected sentence:"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=250)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    match = re.search(r'Corrected sentence:\s*(.*)', decoded)
    corrected_text = match.group(1).strip() if match else decoded.strip()

    return {"corrected": corrected_text}

# Start ngrok tunnel

ngrok.set_auth_token("")

public_url = ngrok.connect(8000)

print("ðŸ”¥ Public API URL:", public_url)

# Start FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)